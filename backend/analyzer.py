"""
AI Analyzer using Google Gemini

Analyzes news articles for:
- Category classification (Crisis, Ocean, Air, Inland, Economy, ETC)
- Sentiment analysis (positive, negative, neutral)
- Country/region extraction
- Keyword extraction
"""

import os
import json
import logging
import time
from typing import List, Dict, Any, Optional
from datetime import datetime

logger = logging.getLogger("analyzer.Gemini")

# Category definitions
CATEGORIES = {
    'Crisis': 'ÌååÏóÖ, ÏÇ¨Í≥†, Î∂ÑÏüÅ, Ïû¨Ìï¥ Îì± ÏúÑÍ∏∞ ÏÉÅÌô©',
    'Ocean': 'Ìï¥Ïö¥, Ïª®ÌÖåÏù¥ÎÑà, Ìï≠Îßå, ÏÑ†Î∞ï Í¥ÄÎ†®',
    'Air': 'Ìï≠Í≥µ ÌôîÎ¨º, Í≥µÌï≠, Ìï≠Í≥µÏÇ¨ Í¥ÄÎ†®',
    'Inland': 'ÎÇ¥Î•ô Ïö¥ÏÜ°, Ìä∏Îü≠, Ï≤†ÎèÑ, Ï∞ΩÍ≥† Í¥ÄÎ†®',
    'Economy': 'Í≤ΩÏ†ú, Ïö¥ÏûÑ, ÏàòÏöî, Î¨¥Ïó≠, Í∏àÏúµ Í¥ÄÎ†®',
    'ETC': 'Í∏∞ÌÉÄ Î¨ºÎ•ò/Í≥µÍ∏âÎßù Îâ¥Ïä§',
}

# Crisis keywords for quick classification
CRISIS_KEYWORDS = [
    'strike', 'crisis', 'disruption', 'closure', 'disaster', 'attack',
    'war', 'conflict', 'shortage', 'congestion', 'delay', 'accident',
    'ÌååÏóÖ', 'ÏúÑÍ∏∞', 'ÌòºÏû°', 'ÏÇ¨Í≥†', 'ÏßÄÏó∞', 'ÌèêÏáÑ', 'Î∂ÑÏüÅ', 'Í≥µÍ≤©', 'Ïû¨Ìï¥',
]

# Negative sentiment keywords
NEGATIVE_KEYWORDS = [
    'decline', 'drop', 'fall', 'crash', 'loss', 'concern', 'risk', 'threat',
    'warning', 'trouble', 'problem', 'failure', 'worst', 'critical',
    'ÌïòÎùΩ', 'Í∞êÏÜå', 'ÏúÑÌóò', 'Ïö∞Î†§', 'ÏÜêÏã§', 'Î¨∏Ï†ú', 'ÏïÖÌôî', 'ÏµúÏïÖ', 'ÏúÑÍ∏∞',
]


class GeminiAnalyzer:
    """
    Analyzes news articles using Google Gemini AI.
    Falls back to rule-based analysis if API is unavailable.
    """
    
    def __init__(self, api_key: str = None):
        """
        Initialize Gemini analyzer.
        
        Args:
            api_key: Gemini API key (uses env var if not provided)
        """
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        self.model = None
        self._init_gemini()
        
        self.stats = {
            'total_analyzed': 0,
            'ai_analyzed': 0,
            'rule_analyzed': 0,
            'errors': 0,
        }
    
    def _init_gemini(self):
        """Initialize Gemini model"""
        if not self.api_key:
            logger.warning("‚ö†Ô∏è GEMINI_API_KEY not set. Using rule-based analysis only.")
            return
        
        try:
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-2.0-flash')
            logger.info("‚úÖ Gemini model initialized successfully")
        except ImportError:
            logger.warning("‚ö†Ô∏è google-generativeai not installed. Using rule-based analysis.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to initialize Gemini: {e}")
    
    def analyze_articles(self, articles: List[Dict[str, Any]], batch_size: int = 20) -> List[Dict[str, Any]]:
        """
        Analyze multiple articles with parallel processing.
        
        Args:
            articles: List of article dictionaries
            batch_size: Number of articles to process in parallel
            
        Returns:
            List of analyzed article dictionaries
        """
        logger.info(f"{'='*60}")
        logger.info(f"ü§ñ Starting AI Analysis (Parallel Processing)")
        logger.info(f"   Total articles: {len(articles)}")
        logger.info(f"{'='*60}")
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def analyze_batch(batch_articles, start_idx):
            """Analyze a batch of articles"""
            batch_results = []
            for idx, article in enumerate(batch_articles):
                try:
                    analyzed_article = self._analyze_single(article)
                    batch_results.append((start_idx + idx, analyzed_article))
                except Exception as e:
                    logger.debug(f"Analysis error for article {start_idx + idx}: {e}")
                    # Keep original article with default values
                    article['category'] = 'ETC'
                    article['sentiment'] = 'neutral'
                    article['is_crisis'] = False
                    article['country_tags'] = []
                    article['keywords'] = []
                    batch_results.append((start_idx + idx, article))
            return batch_results
        
        # Process articles in parallel batches
        analyzed = [None] * len(articles)
        total_processed = 0
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for i in range(0, len(articles), batch_size):
                batch = articles[i:i+batch_size]
                future = executor.submit(analyze_batch, batch, i)
                futures.append(future)
            
            for future in as_completed(futures):
                batch_results = future.result()
                for idx, result in batch_results:
                    analyzed[idx] = result
                    self.stats['total_analyzed'] += 1
                    total_processed += 1
                    
                    if total_processed % 50 == 0:
                        logger.info(f"   Analyzing... {total_processed}/{len(articles)}")
        
        # Filter out None values (shouldn't happen, but safety check)
        analyzed = [a for a in analyzed if a is not None]
        
        logger.info(f"{'='*60}")
        logger.info(f"‚úÖ Analysis complete")
        logger.info(f"   Total: {self.stats['total_analyzed']}")
        logger.info(f"   AI analyzed: {self.stats['ai_analyzed']}")
        logger.info(f"   Rule-based: {self.stats['rule_analyzed']}")
        logger.info(f"   Errors: {self.stats['errors']}")
        logger.info(f"{'='*60}")
        
        return analyzed
    
    def _analyze_single(self, article: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze a single article"""
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        text = f"{title} {summary}".lower()
        
        # Try AI analysis first, fall back to rules
        if self.model:
            try:
                result = self._analyze_with_ai(article)
                if result:
                    self.stats['ai_analyzed'] += 1
                    return result
            except Exception as e:
                logger.debug(f"AI analysis failed, using rules: {e}")
        
        # Rule-based analysis
        self.stats['rule_analyzed'] += 1
        return self._analyze_with_rules(article, text)
    
    def _analyze_with_ai(self, article: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Analyze article using Gemini AI"""
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        
        prompt = f"""Analyze this logistics/supply chain news article and provide a JSON response:

Title: {title}
Summary: {summary}

Respond with ONLY a JSON object (no markdown, no explanation):
{{
    "category": "one of: Crisis, Ocean, Air, Inland, Economy, ETC",
    "sentiment": "one of: positive, negative, neutral",
    "is_crisis": true or false,
    "country_tags": ["ISO country codes mentioned, e.g., US, KR, CN"],
    "keywords": ["3-5 key terms from the article"]
}}

Categories:
- Crisis: Strikes, accidents, conflicts, disasters (actual ongoing incidents)
- Ocean: Maritime shipping, containers, ports, shipbuilding, marine research, KRISO
- Air: Air cargo, airports, airlines
- Inland: Trucking, rail, warehousing
- Economy: Economic indicators, freight rates, trade
- ETC: Other logistics news

IMPORTANT: Technology development, R&D success, system innovation news should NOT be classified as Crisis.
For example, "AI-based damage control system development success" is Ocean, not Crisis."""

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # Clean up response
            if text.startswith('```'):
                text = text.split('\n', 1)[1]
                text = text.rsplit('```', 1)[0]
            
            result = json.loads(text)
            
            # Merge with original article
            article['category'] = result.get('category', 'ETC')
            article['sentiment'] = result.get('sentiment', 'neutral')
            article['is_crisis'] = result.get('is_crisis', False)
            article['country_tags'] = result.get('country_tags', [])
            article['keywords'] = result.get('keywords', [])
            
            # Rate limiting for Gemini API
            time.sleep(0.1)
            
            return article
            
        except json.JSONDecodeError:
            logger.debug("Failed to parse AI response as JSON")
            return None
        except Exception as e:
            logger.debug(f"AI analysis error: {e}")
            return None
    
    def _analyze_with_rules(self, article: Dict[str, Any], text: str) -> Dict[str, Any]:
        """Analyze article using rule-based approach"""
        
        # Category classification
        category = self._classify_category(text)
        article['category'] = category
        
        # Sentiment analysis
        sentiment = self._classify_sentiment(text)
        article['sentiment'] = sentiment
        
        # Crisis detection
        is_crisis = category == 'Crisis' or any(kw in text for kw in CRISIS_KEYWORDS)
        article['is_crisis'] = is_crisis
        
        # Country extraction (simple)
        article['country_tags'] = self._extract_countries(text)
        
        # Keyword extraction (simple)
        article['keywords'] = self._extract_keywords(text)
        
        return article
    
    def _classify_category(self, text: str) -> str:
        """Rule-based category classification"""
        text_lower = text.lower()
        
        # Technology/Development keywords (not Crisis)
        tech_positive_keywords = ['Íµ≠ÏÇ∞Ìôî', 'ÏÑ±Í≥µ', 'Í∞úÎ∞ú', 'Í∏∞Ïà†', 'ÏãúÏä§ÌÖú', 'development', 
                                  'technology', 'innovation', 'research', 'Ïó∞Íµ¨', 'ÌòÅÏã†']
        has_tech_positive = any(kw in text_lower for kw in tech_positive_keywords)
        
        # Ocean/Maritime (check before Crisis to prioritize domain)
        ocean_keywords = ['ship', 'port', 'container', 'maritime', 'vessel', 'cargo ship',
                         'ÏÑ†Î∞ï', 'Ìï≠Îßå', 'Ïª®ÌÖåÏù¥ÎÑà', 'Ìï¥Ïö¥', 'ÏÑ†ÏÇ¨', 'kriso', 'Ìï¥Ïñë', 
                         'ÏÜêÏÉÅÌÜµÏ†ú', 'Ï°∞ÏÑ†', 'Ìï¥ÏÇ¨', 'Ìï¥ÏàòÎ∂Ä']
        if any(kw in text_lower for kw in ocean_keywords):
            # If it's a tech/development news in ocean domain, it's Ocean, not Crisis
            if has_tech_positive:
                return 'Ocean'
            # Check if it's actually a crisis in ocean domain
            if any(kw in text_lower for kw in CRISIS_KEYWORDS):
                return 'Crisis'
            return 'Ocean'
        
        # Crisis indicators (only if not tech/development news)
        if not has_tech_positive and any(kw in text_lower for kw in CRISIS_KEYWORDS):
            return 'Crisis'
        
        # Air
        air_keywords = ['air cargo', 'airport', 'airline', 'flight', 'aviation',
                       'Ìï≠Í≥µ', 'Í≥µÌï≠', 'ÌôîÎ¨ºÍ∏∞']
        if any(kw in text_lower for kw in air_keywords):
            return 'Air'
        
        # Inland
        inland_keywords = ['truck', 'rail', 'warehouse', 'distribution', 'last mile',
                          'Ìä∏Îü≠', 'Ï≤†ÎèÑ', 'Ï∞ΩÍ≥†', 'Î¨ºÎ•òÏÑºÌÑ∞', 'Î∞∞ÏÜ°']
        if any(kw in text_lower for kw in inland_keywords):
            return 'Inland'
        
        # Economy
        economy_keywords = ['rate', 'price', 'cost', 'trade', 'economy', 'tariff', 'gdp',
                           'Ïö¥ÏûÑ', 'ÏöîÍ∏à', 'Î¨¥Ïó≠', 'Í≤ΩÏ†ú', 'Í¥ÄÏÑ∏']
        if any(kw in text_lower for kw in economy_keywords):
            return 'Economy'
        
        return 'ETC'
    
    def _classify_sentiment(self, text: str) -> str:
        """Rule-based sentiment classification"""
        text_lower = text.lower()
        
        negative_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in text_lower)
        
        positive_keywords = ['growth', 'increase', 'rise', 'recovery', 'improve', 'success',
                            'ÏÑ±Ïû•', 'Ï¶ùÍ∞Ä', 'ÏÉÅÏäπ', 'ÌöåÎ≥µ', 'Í∞úÏÑ†', 'Ìò∏Ï°∞']
        positive_count = sum(1 for kw in positive_keywords if kw in text_lower)
        
        if negative_count > positive_count:
            return 'negative'
        elif positive_count > negative_count:
            return 'positive'
        return 'neutral'
    
    def _extract_countries(self, text: str) -> List[str]:
        """Extract country codes from text"""
        text_upper = text.upper()
        
        country_mapping = {
            'UNITED STATES': 'US', 'USA': 'US', 'AMERICA': 'US', 'ÎØ∏Íµ≠': 'US',
            'CHINA': 'CN', 'CHINESE': 'CN', 'Ï§ëÍµ≠': 'CN',
            'KOREA': 'KR', 'KOREAN': 'KR', 'ÌïúÍµ≠': 'KR',
            'JAPAN': 'JP', 'JAPANESE': 'JP', 'ÏùºÎ≥∏': 'JP',
            'GERMANY': 'DE', 'GERMAN': 'DE', 'ÎèÖÏùº': 'DE',
            'SINGAPORE': 'SG', 'Ïã±Í∞ÄÌè¨Î•¥': 'SG',
            'TAIWAN': 'TW', 'ÎåÄÎßå': 'TW',
            'VIETNAM': 'VN', 'Î≤†Ìä∏ÎÇ®': 'VN',
            'INDIA': 'IN', 'Ïù∏ÎèÑ': 'IN',
            'NETHERLANDS': 'NL', 'DUTCH': 'NL', 'ÎÑ§ÎçúÎûÄÎìú': 'NL',
            'UK': 'GB', 'BRITAIN': 'GB', 'BRITISH': 'GB', 'ÏòÅÍµ≠': 'GB',
            'FRANCE': 'FR', 'FRENCH': 'FR', 'ÌîÑÎûëÏä§': 'FR',
            'RUSSIA': 'RU', 'RUSSIAN': 'RU', 'Îü¨ÏãúÏïÑ': 'RU',
            'UKRAINE': 'UA', 'Ïö∞ÌÅ¨ÎùºÏù¥ÎÇò': 'UA',
            'IRAN': 'IR', 'Ïù¥ÎûÄ': 'IR',
            'SAUDI': 'SA', 'ÏÇ¨Ïö∞Îîî': 'SA',
            'UAE': 'AE', 'ÏïÑÎûçÏóêÎØ∏Î¶¨Ìä∏': 'AE',
            'YEMEN': 'YE', 'ÏòàÎ©ò': 'YE',
        }
        
        found = set()
        for keyword, code in country_mapping.items():
            if keyword in text_upper:
                found.add(code)
        
        return list(found)[:5]  # Limit to 5 countries
    
    def _extract_keywords(self, text: str) -> List[str]:
        """Extract keywords from text (simple approach)"""
        # Common logistics keywords to look for
        keywords_to_check = [
            'strike', 'port', 'shipping', 'freight', 'container', 'delay',
            'disruption', 'supply chain', 'logistics', 'cargo', 'tariff',
            'trade', 'export', 'import', 'crisis', 'congestion',
            'ÌååÏóÖ', 'Ìï≠Îßå', 'Ìï¥Ïö¥', 'Î¨ºÎ•ò', 'Ïª®ÌÖåÏù¥ÎÑà', 'ÏßÄÏó∞', 'ÏúÑÍ∏∞',
        ]
        
        text_lower = text.lower()
        found = [kw for kw in keywords_to_check if kw in text_lower]
        
        return found[:10]  # Limit to 10 keywords
    
    def generate_insights(self, article: Dict[str, Any]) -> Dict[str, str]:
        """
        Generate trade/logistics/SCM insights for a headline article.
        
        Args:
            article: Article dictionary with title and content_summary
            
        Returns:
            Dictionary with 'trade', 'logistics', 'scm' insights
        """
        title = article.get('title', '')
        summary = article.get('content_summary', '')
        
        if self.model:
            try:
                return self._generate_insights_with_ai(title, summary)
            except Exception as e:
                logger.debug(f"AI insights generation failed: {e}")
        
        # Rule-based fallback
        return self._generate_insights_with_rules(title, summary)
    
    def _generate_insights_with_ai(self, title: str, summary: str) -> Dict[str, str]:
        """Generate insights using Gemini AI - comprehensive summary"""
        prompt = f"""ÎãπÏã†ÏùÄ Î¨¥Ïó≠, Î¨ºÎ•ò, SCM Ï†ÑÎ¨∏ Î∂ÑÏÑùÍ∞ÄÏûÖÎãàÎã§. ÏïÑÎûò Îâ¥Ïä§ Í∏∞ÏÇ¨Î•º ÏùΩÍ≥† Ï¢ÖÌï©Ï†ÅÏù∏ ÏãúÏÇ¨Ï†êÏùÑ 3Ï§ÑÎ°ú ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî.

üì∞ Í∏∞ÏÇ¨ Ï†úÎ™©: {title}
üìù Í∏∞ÏÇ¨ ÏöîÏïΩ: {summary}

ÏöîÏ≤≠ÏÇ¨Ìï≠:
- Î¨¥Ïó≠, Î¨ºÎ•ò, SCM Í¥ÄÏ†êÏùÑ Ï¢ÖÌï©ÌïòÏó¨ Ïù¥ Í∏∞ÏÇ¨Í∞Ä Ï£ºÎäî ÌïµÏã¨ ÏãúÏÇ¨Ï†êÏùÑ 3Ï§ÑÎ°ú ÏûëÏÑ±
- Í∞Å Ï§ÑÏùÄ 30~50Ïûê ÎÇ¥Ïô∏Ïùò ÌïúÍµ≠Ïñ¥ Î¨∏Ïû•
- Íµ¨Ï≤¥Ï†ÅÏù∏ ÏàòÏπò, ÏßÄÏó≠, Í∏∞ÏóÖÎ™Ö, ÏòÅÌñ• Î≤îÏúÑÎ•º Ìè¨Ìï®
- ÏùºÎ∞òÏ†ÅÏù∏ Ï°∞Ïñ∏Ïù¥ ÏïÑÎãå Ïù¥ Í∏∞ÏÇ¨Ïóê ÌäπÌôîÎêú ÎÇ¥Ïö©
- ÌãÄÏóê ÎßûÏ∂îÏßÄ ÎßêÍ≥† ÏûêÏó∞Ïä§ÎüΩÍ≤å Ï¢ÖÌï©Ï†ÅÏúºÎ°ú ÏûëÏÑ±

ÏïÑÎûò JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµ (ÎßàÌÅ¨Îã§Ïö¥, ÏÑ§Î™Ö ÏóÜÏù¥):
{{
    "insight1": "Ï≤´ Î≤àÏß∏ ÏãúÏÇ¨Ï†ê (Î¨¥Ïó≠/Î¨ºÎ•ò/SCM Ï¢ÖÌï©)",
    "insight2": "Îëê Î≤àÏß∏ ÏãúÏÇ¨Ï†ê (Î¨¥Ïó≠/Î¨ºÎ•ò/SCM Ï¢ÖÌï©)",
    "insight3": "ÏÑ∏ Î≤àÏß∏ ÏãúÏÇ¨Ï†ê (Î¨¥Ïó≠/Î¨ºÎ•ò/SCM Ï¢ÖÌï©)"
}}"""

        try:
            response = self.model.generate_content(prompt)
            text = response.text.strip()
            
            # Clean up response
            if text.startswith('```'):
                text = text.split('\n', 1)[1]
                text = text.rsplit('```', 1)[0]
            
            result = json.loads(text)
            time.sleep(0.1)  # Rate limiting
            
            # Í∏∞Ï°¥ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
            return {
                'trade': result.get('insight1', ''),
                'logistics': result.get('insight2', ''),
                'scm': result.get('insight3', ''),
            }
            
        except Exception as e:
            logger.debug(f"AI insights parsing error: {e}")
            return self._generate_insights_with_rules(title, summary)
    
    def _generate_insights_with_rules(self, title: str, summary: str) -> Dict[str, str]:
        """
        Generate insights using rule-based approach.
        Used as fallback when AI is unavailable.
        """
        # Default simple insights when AI is not available
        return {
            'trade': 'Í¥ÄÎ†® ÏãúÏû• ÎèôÌñ• Î™®ÎãàÌÑ∞ÎßÅ Î∞è ÏòÅÌñ• Î∂ÑÏÑù ÌïÑÏöî',
            'logistics': 'Î¨ºÎ•ò ÏùºÏ†ï Î∞è ÎπÑÏö© ÏòÅÌñ• Í≤ÄÌÜ† ÌïÑÏöî',
            'scm': 'Í≥µÍ∏âÎßù Î¶¨Ïä§ÌÅ¨ Í¥ÄÎ¶¨ Ï†êÍ≤Ä Í∂åÏû•',
        }
    
    def _extract_location_from_text(self, text: str) -> str:
        """Extract primary location/port from text"""
        locations = {
            'Î∂ÄÏÇ∞': 'Î∂ÄÏÇ∞Ìï≠', 'Ïù∏Ï≤ú': 'Ïù∏Ï≤úÌï≠', 'Í¥ëÏñë': 'Í¥ëÏñëÌï≠', 'ÌèâÌÉù': 'ÌèâÌÉùÌï≠',
            'busan': 'Î∂ÄÏÇ∞Ìï≠', 'shanghai': 'ÏÉÅÌïòÏù¥Ìï≠', 'singapore': 'Ïã±Í∞ÄÌè¨Î•¥Ìï≠',
            'rotterdam': 'Î°úÌÖåÎ•¥Îã¥Ìï≠', 'los angeles': 'LAÌï≠', 'long beach': 'Î°±ÎπÑÏπòÌï≠',
            'red sea': 'ÌôçÌï¥', 'ÌôçÌï¥': 'ÌôçÌï¥', 'suez': 'ÏàòÏóêÏ¶àÏö¥Ìïò', 'ÏàòÏóêÏ¶à': 'ÏàòÏóêÏ¶àÏö¥Ìïò',
            'panama': 'ÌååÎÇòÎßàÏö¥Ìïò', 'ÌååÎÇòÎßà': 'ÌååÎÇòÎßàÏö¥Ìïò',
            'Ï§ëÍµ≠': 'Ï§ëÍµ≠', 'china': 'Ï§ëÍµ≠', 'ÎØ∏Íµ≠': 'ÎØ∏Íµ≠', 'us': 'ÎØ∏Íµ≠',
            'Ïú†ÎüΩ': 'Ïú†ÎüΩ', 'europe': 'Ïú†ÎüΩ', 'ÏùºÎ≥∏': 'ÏùºÎ≥∏', 'japan': 'ÏùºÎ≥∏',
        }
        
        text_lower = text.lower()
        for keyword, location in locations.items():
            if keyword in text_lower:
                return location
        return ""
    
    def _extract_company_from_text(self, text: str) -> str:
        """Extract primary company/carrier from text"""
        companies = {
            'maersk': 'Maersk', 'msc': 'MSC', 'cosco': 'COSCO', 'cma cgm': 'CMA CGM',
            'evergreen': 'Evergreen', 'hmm': 'HMM', 'one': 'ONE', 'hapag': 'Hapag-Lloyd',
            'Î®∏Ïä§ÌÅ¨': 'Maersk', 'ÏóêÎ≤ÑÍ∑∏Î¶∞': 'Evergreen',
            'fedex': 'FedEx', 'ups': 'UPS', 'dhl': 'DHL',
            'tesla': 'Tesla', 'apple': 'Apple', 'samsung': 'ÏÇºÏÑ±', 'ÏÇºÏÑ±': 'ÏÇºÏÑ±',
            'tsmc': 'TSMC', 'nvidia': 'NVIDIA',
        }
        
        text_lower = text.lower()
        for keyword, company in companies.items():
            if keyword in text_lower:
                return company
        return ""

